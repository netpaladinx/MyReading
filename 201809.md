# My Reading & Guess

#### 2018.09

1. (2016) **Revisiting Semi-Supervised Learning with Graph Embeddings** (Yang et al.)
    - Guess: Traditional machine learning methods are always beautiful and even perfect in the mathematic form, often based on some pure assumption. The only problem is that the real world is not your ideal world, always noisy and cluttered. For example, "nearby nodes in a graph tend to have the same labels", but why should it be like that? Maybe someone would say "statistically". Of course it is, but could we do anything more than just saying "statistically"? I think the statistics-style thinking is like a hammer, but what we need is a scalpel, even then we hand the scalpel to a black box. Then, we could say that "maybe nearby nodes in a graph tend to have something similar", a milder expression. That is exactly the background where the embedding-based approach with the unsupervised framework comes out, replacing the regularization based on some arbitrary assumption for the supervised setting. The path to the output layer for a supervised loss should not be interfered by regularization, and only the embeddings should be regularized or trained for some unsupervised loss.
2. (2014) **Spectral Networks and Deep Locally Connected Networks on Graphs** (Bruna et al.)
    - Guess: Coordinates, dimensions, neurons, and units are different terms of the same thing. From grids to general graphs, we find three structure properties matter: (1) translation structure with weight sharing (2) compactly supported filters (much smaller support) (3) multiscale dyadic clustering. But still, we need some graph that already exists. I want to ask could we apply the same or similar way to unobserved structure (as unknown structural parameters), and force some underlying structure emerge as a result of imposing the above convolutional way. For example, impose a CNN layer on output of a fully-connected layer. Although we know there is no any spatial geometry along dimensions of fc's output, I just wonder the possibility of forcing the output to match on a low-dimensional grid. Or more generally, we figure out some way that can learn the underlying grid or graph structure under some following convolution operation. Given multiple dimensions without any structure information, we know at least that they should be grouped (overlapped or non-overlapped) with discriminative with-group roles (for weight-sharing templates) and increasing channels.
3. (2016) **Discriminative Embeddings of Latent Variable Models for Structured Data** (Dai et al.)
    - Guess: There is a counterpart in embedding spaces for probability distribution space, and a counterpart in nonlinear mapping computation for typical probability operations. All our estabilished probability theories, concepts and equations, plus any fantasy about it, will be squashed into basic arthimetic computations following some sequential procedure. Until then, we will find same issues occuring as in normal deep learning computation. For example, the sophiscated variational inference procedure would become a special case of knowledge distillation, transferring distribution from a complicated coupled function product system into a more concise function product system with certain informaitnon loss. 
