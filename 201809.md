# My Reading & Guess

#### 2018.09

1. (2016) *Revisiting Semi-Supervised Learning with Graph Embeddings* (Yang et al.)
  - Guess: Traditional machine learning methods are always beautiful and even perfect in the mathematic form, often based on some pure assumption. The only problem is that the real world is not your ideal world, always noisy and cluttered. For example, "nearby nodes in a graph tend to have the same labels", but why should it be like that? Maybe someone would say "statistically". Of course it is, but could we do anything more than just saying "statistically"? I think the statistics-style thinking is like a hammer, but what we need is a scalpel, even then we hand the scalpel to a black box. Then, we could say that "maybe nearby nodes in a graph tend to have something similar", a milder expression. That is exactly the background where the embedding-based approach with the unsupervised framework comes out, replacing the regularization based on some arbitrary assumption for the supervised setting.
