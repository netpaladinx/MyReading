# My Reading & Guess

#### 2018.8

1. (2018) **Manifold Mixup: Encouraging Meaningful On-Manifold Interpolation as a Regularizer**
    - Guess: The nature of generalization is linearization, and the purpose for performing all kinds of nonlinear transformations is to help the linearity find the underlying space and give a fully play to the talent of the linearity. The nonlinearity is the path but not the destination, since it is the destination for the linearity. When the destination reached, the manifold would fill all the space.
2. (2016) **Progressive Neural Networks**
    - Guess: Multi-paths or multi-braches structures based on extensive skip connections, lateral connections, highways and gates, plus a procedure to actively search and build such structures, by exploiting a more effective and iterative, implicit or explicit attention mechanism, might be one of the way to avoid catastrophic forgetting and interference problem and simultaneously leverage learned and transferred knowledge.
3. (2017) **Measuring the tendency of CNNs to Learn Surface Statistical Regularities**
    - Guess: The space of high-level abstraction cannot be learned by using SCG when reaching the first-time convergence, except that only statistical regularities can be captured in this way. This level of abstraction can only be sought in a back-and-forth, twist-and-turns and chaos-and-consciousness process, finally leaving the loop and reaching some enlightment point. That means there cannot be just one objective, one stationary optimization direction.
